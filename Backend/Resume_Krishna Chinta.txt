Krishna Chinta+1 940-448-0587________________________________________________________________________________________Professional Summary  9 + Years of Experience with a strong background in data modeling, database development, and data analysis.  Proficient in creating conceptual, logical, and physical data models for efficient data representation and analysis.  Familiarity with Hugging face models, Open AI. Experimented hugging face models for recommendation system.  Experience in database development, including stored procedures, functions, and other database objects ensuring dataintegrity and efficient data processing.  Designed, implemented, and maintained CI/CD pipelines for various software projects, ensuring smooth and automatedbuild, test, and deployment processes.  Expert in writing complex SQL queries and optimizing them to extract valuable insights from the data.  Familiarity with AWS, Microsoft Azure, GCP and their services for implementing data pipelines and conducting customeranalytics and predictions.  Implemented end-to-end data processing pipelines using Apache Spark for large-scale data analysis and machine learningtasks. Developed predictive models using Python to assist decision-making processes in business.  Experience with IaC tools like AWS CloudFormation, Terraform, AWS CDK to provision and manage infrastructure.  Collaborated with cross-functional teams to formulate hypotheses related to market trends and customer preferences,leading to data-driven.  Collaborated on Python-based projects using version control systems like Git for team-based data science initiatives.  Experience in applying Gen AI techniques to enhance model performance and generate actionable insights.  Skilled in using Business Objects, MS Excel Reports, MS Access Reports, and Tableau reports for visualizing data trends.  Proficiency in TensorFlow's high-level APIs such as Keras to streamline the development of complex neural networkarchitectures. Familiarity with using HPCs, GPUs.  Designed and optimized Hive queries for efficient processing of large-scale datasets, showcasing proficiency in HiveQL.Integrated Hive with external data sources and data lakes for seamless data access and analysis across diverse datasets.  Demonstrated expertise in the Hadoop ecosystem, including HDFS, MapReduce for distributed storage and processing.  Designed and implemented NLP algorithms in Python for tasks such as text classification, named entity recognition, andsentiment analysis. Familiarity with NLTK, Spacy for extracting meaningful information from unstructured text data.  Leveraged PyTorch's GPU acceleration to significantly speed up training and inference for large-scale models.  Used Jenkins pipelines to drive all micro services builds out to the Docker registry and then deployed to Kubernetes. CreatedPods and managed using Kubernetes.  Exposure to AI and deep learning platforms/methodologies such as TensorFlow, RNN, and LSTM.  Proficient in data visualization techniques using tools like Data Studio, Tableau, Power BI and QlikView  Experience in implementing various machine learning algorithms, including regression, clustering, classification.  Strong programming skills in Python, utilizing libraries like Pandas, NumPy, SciPy, Matplotlib, OpenCV, Tensorflow  Skilled in ensuring data quality, performing anomaly detection, and estimation using numerical optimization and statisticalmethods.Technical Skills:Data Science SkillsDatabaseToolsOperating SystemsLanguagesML skillsCloud TechnologyMachine Learning, Deep Learning, Data Mining, Data Analysis, Big data, Visualizing, DataModelling, Reinforcement learning, LLMMicrosoft, SQL, PostgreSQL, MongoDB, MySQL, Cosmos DBBusiness Objects, MS Excel Reports, MS Access Reports, Data Studio, Tableau reports,Power BI, QlikView, Jupyter notebook, Colab, GISWindows, LinuxSQL, Python (Pandas, SciPy, Matplotlib, TensorFlow, OpenCV, PyTesseract, Django, Flask),R, Java, Html, CSS, JavaScript, Tensorflow, PytorchRegression, Clustering, Classification, PCA, Forecasting, Neural networks, NLP, LLMMicrosoft Azure, GCP, Spark, Hadoop  Education Details: Master's in business Analytics- UNT, 2019Bachelor's in computer science -Loyola, 2013Professional Experience:Client: QVC, West Chester, PAAug 21 - Till DateRole: Sr. Machine Learning EngineerResponsibilities:  Experienced with working on various Machine Learning techniques and algorithms with tasks like Exploratory Dataanalysis (EDA), Feature Engineering, Model Development and Scaling of ML Models using Python, Numpy, Pandas andPyTorch.  Developed an LSTM neural network for analyzing text data, including item descriptions and customer comments, withinthe context of a retail project. Proficient in training artificial intelligence chatbots for retail applications.  Created a deep neural network that integrates LSTM model output with other relevant features to enhance retail-relatedprojects.  Developed Spark applications in PySpark for distributed data processing, demonstrating proficiency in both batch andstreaming processing.  Utilized Dax functions to transform data columns and visualized related information in PowerBI  Automated demand forecasting with MlOps in Azure. Leveraged Azure data explorer for data manipulation and featurecreation.  Developed an MLOps pipeline that automates the process of demand forecasting using machine learning models.  Developed and implemented machine learning models using Python, leveraging frameworks like TensorFlow andPyTorch.  Used LLM and Generative AI to automatically generate product descriptions for retail items.  Developed and implemented deep learning models using TensorFlow for tasks such as natural language processing, andtime-series analysis.  Implemented RESTful APIs for communication between microservices, ensuring seamless integration.  Conducted scalability assessments and devised strategies to accommodate future growth and increased data volumes,ensuring the microservices architecture is scalable to meet the evolving demands.  Utilized machine learning techniques (e.g., ARIMA, XGBoost) for demand forecasting.  Utilized Hadoop for processing and analyzing large-scale datasets, showcasing proficiency in handling big data.  Prioritized user needs and market demands by incorporating customer feedback and behavior data into the productroadmap optimization process and established key performance indicators (KPIs) to measure the success of theoptimized product roadmap, such as increased feature adoption rates and improved customer satisfaction scores.  Integrated a feedback loop mechanism that allowed for constant monitoring of feature performance and quickadjustments to the roadmap as needed.  Leveraged Langchain's Large Language Models (LLMs) to analyze customer reviews, product descriptions, and pastpurchase history. Designed a chain of prompts where the system first suggests products and then asks clarifyingquestions to refine recommendations based on the user's preferences.  Integrated machine learning models into microservices architecture to provide intelligent features.  Conducted data profiling and exploration using Hive to gain insights into the structure and content of datasets.  Developed Docker container based micro services using Python Fast API and deployment on Kubernetes cluster.Designed APIs for seamless interaction between microservices and machine learning components.  Utilized TensorFlow for model deployment, integrating machine learning models into production environments for realtimepredictions.  Developed and optimized MapReduce programs in Hadoop for parallel processing and distributed computing tasks.  Developed a chatbot powered by GPT-3.5(LLM type) to handle customer inquiries in online retail setting and created agenerative AI model that generates compelling product descriptions based on existing product details and attributes.Extended the chatbot's capabilities to provide personalized product recommendations.  Trained the chatbot to maintain context during conversations by utilizing the long-form language model capabilities ofGPT-3.5, allowing for more coherent and contextually relevant interactions with customers. Utilized pgVector to storepre-computed vector embeddings of product data.  Established a feedback loop mechanism to continually improve the generative AI models by incorporating user feedbackinto the training process, allowing the system to adapt and evolve based on real-world interactions.  Implemented monitoring tools to track the performance of the chatbot and generative AI models, optimizing them overtime to enhance accuracy, response time, and overall customer satisfaction.  Updated product recommendation system with Graph neural networks (GNN). Experimented with graph convolutionnetworks and Attention networks.  Implemented Gen AI to optimize pricing strategies based on factors such as market competition, demand elasticity, andseasonality and automatically adjusted prices in real-time to maximize revenue and profit margins.  Gathered, documented, and implemented business requirements for analysis or as part of a long-term document/reportgeneration. Analyzed large volumes of data and provided results to technical and managerial staff.  Worked with various data pools and DBAs to have access to data.  Addressed ethical considerations in the deployment of generative AI by implementing safeguards to avoid biased orinappropriate content generation and adhering to best practices for responsible AI use in customer interactions.  Worked on Numerical optimization, Anomaly Detection and estimation, A/B testing, Statistics, NumPy, SciPy, Pandas,scikit-learn.  Utilized Spark SQL to query structured data and performed complex data manipulations on distributed datasets.  Used NLTK , Spacy, Text blob libraries with python to perform sentiment analysis for customer data and Gen AIalgorithms are employed to analyze the integrated data and generate actionable insights.  Used K-fold cross Validation technique to improve model performance and to test the model on the sample data beforefinalizing the model.  Conducted hyperparameter tuning using TensorFlow's capabilities, optimizing model performance for accuracy,precision, recall, and other relevant metrics using Scikit learn .  Utilized Azure Cognitive Services for extracting structured data from retail documents like invoices, receipts, andpurchase orders. Used the extracted data to update the knowledge graph and improve data accuracy.  Investigated if there are statistically significant differences in the spending patterns of different customer segments andperformed Test hypotheses related to customer demographics and purchasing behavior.  Implemented real-time data processing and analytics using Spark Streaming, ensuring low-latency insights fromstreaming data sources.  Used query languages such as SQL and experience with NoSQL databases, such as MongoDB, GrapghDB.  Worked with both unstructured/structured data Machine Learning Algorithms such as Linear, Logistic, Decision Tress,Random Forests, Support Vector Machines, Neural Networks, KNN, and Time series analysis.  Automated model selection and hyper parameter tuning using Azure autoMLClient: J.D. POWERAug 19   July 21Role: Sr. Data Scientist/ ML EngineerResponsibilities:  Implemented various Machine learning algorithms - Linear Regression, Logistic Regression, Decision Tree, SVM, NaiveBayes, KNN, K-Means, Random Forest, and Gradient Boost & Adaboost and deployed into production on GCP, Azure.  Built data pipeline framework with python for data extraction, data wrangling and data loading using services like googlecloud data flow.  Designed and implemented scalable and reliable cloud infrastructure on GCP to support data science workflows.  Collected customer feedback through JSON-based survey APIs. Utilized customer data in JSON format to createpersonalized marketing campaigns. Designed Bash scripts for customer segmentation and message delivery.  Stored survey responses in MongoDB for structured data and utilize a Vector database for handling high-dimensionaldata. Applied embedding models for sentiment analysis on open-ended responses to gauge sentiments.  Developed interactive dashboards using Data Studio to visualize store performance metrics, such as sales, foot traffic,and customer satisfaction.  Applied TensorFlow for implementing and deploying computer vision models, including object detection, segmentation,and image generation.  Developed and maintained Hive meta store for centralized metadata management, ensuring consistency and accuracyacross the data ecosystem.  Designed and deployed Spark applications on large clusters, managing resources effectively for scalability and reliability.  Developed serverless computing solutions using GCP Cloud Functions, enabling event-triggered data processing tasks.  Proficient in using Python to interact with various databases, such as MySQL, PostgreSQL, and MongoDB.  Developed business predictive/historic analysis, Data Mining/Text Mining using Python on various platforms (Jupyternotebook, Google Colab).  Utilized TensorFlow for natural language processing tasks, including text classification, sentiment analysis, and languagetranslation.  Applied NLP techniques in domain-specific contexts, tailoring models for industry-specific jargon and requirements.  Integrated new tools and developed technology frameworks/prototypes to accelerate the data integration process andempower the deployment of predictive analytics by developing Spark Scala modules with R.  Executed ETL processes in Hadoop, extracting data from various sources, transforming it, and loading it into Hadoop foranalysis.  Worked on feature engineering and extraction using Spark to enhance the predictive power of machine learning models.  Worked on cost optimization, explained how you've managed AWS billing, utilized AWS Cost Explorer, and implementedcost-saving strategies. Developed Docker container based micro services and deployment on Kubernetes cluster.  Wrote several Teradata SQL Queries using Teradata SQL Assistant for Ad Hoc Data Pull request.  Used Python, NumPy, Pandas, and PyTorch in data analysis (EDA), feature engineering, model development, and MLmodel scaling.  Architected custom solutions for data visualization using tools like tableau, Packages in Python, R and R-Shiny.  Implemented data compression techniques in Hadoop (using codecs like Snappy) to optimize storage and improveprocessing speed.  Implemented big data processing using Azure Databricks, enabling distributed computing for large-scale data analysis.  Created hypotheses on market trends and client preferences in conjunction with cross-functional teams, resulting indata-driven decision-making.  Implemented Spark-based ETL processes for transforming raw data into a structured format suitable for analysis andmodeling. Designed and developed ETL processes for dimension and fact file creation.  Stayed current with the latest TensorFlow releases and incorporated new features and improvements into machinelearning workflows.  Collaborated on Python-based projects using version control systems like Git for team-based data science initiatives.  Primary liaison between customer and engineering groups; served as the key unifying force for all BI platform activitiesthat enables better communication across teams, proactively identifies gaps, and ensured the successful delivery ofcapabilities.Client: Syntel, IndiaNov  15   Mar 18Role: Data Scientist / ML EngineerResponsibilities:  Worked with business requirements analysts/subject matter experts to identify and understand requirements.Conducted user interviews and data analysis review meetings.  Defined key facts and dimensions necessary to support the business requirements along with Data Modeler.  Created draft data models for understanding and to help Data Modeler.  Resolved the data related issues such as: assessing data quality, data consolidation, evaluating existing data sources.  Manipulated, cleaned & processed data using Excel, Access and SQL.  Identified KPIs and created dashboards using Power BI along with DAX.  Participated in TensorFlow-related training and workshops to enhance team knowledge and proficiency with theframework.  Integrated machine learning models into microservices for real-time predictions and decision-making.  Conducted data model reviews with project team members captured technical metadata through data modeling tools  Worked on Hadoop's structure, encompassing its components like HDFS, Job Tracker, Task Tracker, Name Node, DataNode, Secondary Name Node, as well as a strong grasp of MapReduce principles.  Implemented security measures, including token-based authentication and encryption, to secure microservicescommunication.  Utilized Spark Data Frames, Spark-SQL, and Spark MLLib for developing and designing proof-of-concepts (POCs) andemployed Scala, Spark SQL, and MLlib libraries. Applied data quality validation techniques to verify critical data elements(CDE) and detected various irregularities in the data.Client: OSI Technologies, IndiaMay 13   Oct 15Role: Data AnalystResponsibilities:  Involved in Analysis of business requirements, gathering functional requirements and create technical designdocumentation.  Responsible for designing, building, and testing workflows in SSIS.  Experienced in Data Analysis, Statistical Analysis, outlier detection/ anomaly using R programming.  Created and developed POCs and applications as per requirements.  Worked on training the data models, data cleaning and validation in Python.  Developed classification data models in Python by pre-processing the data, cleansing, validating, and testing the data.-----END OF RESUME-----

Name: Krishna Chinta
Santa Rosa, CA, 95401 US
Phone: +19404480587
Email: krishhnaa.85@gmail.com

SUMMARY
Resume Title: UploadedProfile-8a5a0c2e-4fac-4d68-af58-193edb0d6cd9
Security Clearance Level: None
Contact Preference: Email

Recent Position Title: Sr. Machine Learning Engineer

Willing to Relocate: No

Downloaded Resume from Monster
s82vanadg87v37dv